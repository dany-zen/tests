#Posterior al entrenamiento, solo para los clientes maliciosos.
import my_models_attacks.moda.data_wm_process as data_pros
import my_models_attacks.moda.models_for_moda as models_moda
import numpy as np
import torch
import torch.nn as nn
from itertools import cycle
from copy import deepcopy

num_epoch=1

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps" if torch.backends.mps.is_available() else "cpu"
)

def moda_asign_wm_data(flex_data, data_name):#Si no se corresponde hacer un listado con la cantidad de elementos  y poner en el lugar de la i y ya
     copy_data = deepcopy(flex_data)
     list_client=[]
     for client_id in list(copy_data.keys()):#No estoy seguro si es así pero bueno
          client = copy_data[client_id]
          list_client.append(client)
     wm_client_data = data_pros.assing_wm_for_each_client(list_client, data_name)
     i=0
     for client_id in list(copy_data.keys()):
          copy_data[client_id] = wm_client_data[i]
          i+=1
          
     return copy_data

def moda_attack_first_moment(data_name, client_model, data, data_clean_client, n_clases):
    wm_data = data_pros.load_wm_data(data_name)
    adv_mi_dataset, train_loader, adv_loader = data_pros.assing_wm_adv(data_clean_client, data, wm_data)
    D, G = models_moda.get_models(data_name = data_name, base_model_D = client_model, n_class= n_clases, dim_input = np.array(data.X_data[0]).shape)
    #Hay que ver si con solo poner el discriminador y extender la capa el toma los valores de los parámetros, lo cual debería hacer
    optimizerG = torch.optim.Adam(G.parameters(), 0.0002, betas=(0.5, 0.999))
    optimizerD = torch.optim.SGD(filter(lambda p: p.requires_grad, D.parameters()),lr= 0.1, momentum=0.9, weight_decay = 0.25)
    criterion_adv = nn.BCELoss()
    criterion_aux = nn.CrossEntropyLoss()

    total_step = len(train_loader)

    for epoch in range(num_epoch):
        for batch_idx, data in enumerate(zip(train_loader,cycle(adv_loader))):
            print("Batch",batch_idx)
            x, target = data[0]#Desde aquí
            images = x.to(device)
            target = torch.LongTensor(target).to(device)
            # TRAIN D
            # On true data in collaborative learning
            froze_layer(D.l_gan_logit)
            predictR, predictRLabel = D(images) #image from the real dataset
            loss_real_aux = criterion_aux(predictRLabel, target)
            optimizerD.zero_grad()
            optimizerG.zero_grad()
            loss_real_aux.backward()
            optimizerD.step()
            real_score = predictR #Hasta aquí esto es como el entrenamiento normal del cliente, creo que se puede ir sin lío
            
            activate_layer(D.l_gan_logit)#A partir de aquí es donde comienzar a tocar el adversario con la muestra de datos que tiene

            x, target = data[1]
            images = x.to(device)
            target = torch.LongTensor(target).to(device)

            current_batchSize = images.size()[0]
            realLabel = torch.ones(current_batchSize).to(device)
            fakeLabel = torch.zeros(current_batchSize).to(device)



            predictR, predictRLabel = D(images)

            print("Tamaño del label virtual", realLabel.size())
            print("Tamaño de la label real predicha", torch.max(predictR)) #predictR.size()
            print("Tamaño de la label real predicha RL", predictRLabel.size()) 
            print("Tamaño del label de imagen real", target.size())

            loss_real_aux = criterion_aux(predictRLabel, target)
            loss_real_adv = criterion_adv(predictR, realLabel)
            real_score = predictR

            latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)#El generador crea imágenes falsas
            gen_labels = torch.LongTensor(np.random.randint(0, n_clases, current_batchSize)).to(device)
            fake_images = G(latent_value , gen_labels) #generate a fake image
            predictF, predictFLabel = D(fake_images)
            loss_fake_adv = criterion_adv(predictF ,  fakeLabel) # compare vs label =0 (D is supposed to "understand" that the image generated by G is fake)
            loss_fake_aux = criterion_aux(predictFLabel, gen_labels)
            fake_score = predictF

            lossD = loss_real_adv + loss_real_aux  + loss_fake_adv + loss_fake_aux

            optimizerD.zero_grad()
            optimizerG.zero_grad()
            lossD.backward()
            optimizerD.step()

            for i in range(6): 
                # TRAIN G
                    latent_value = torch.normal(0,10,(current_batchSize, 128)).to(device)
                    gen_labels = torch.LongTensor(np.random.randint(0, n_clases, current_batchSize)).to(device)
                    fake_images= G(latent_value, gen_labels) #Generate a fake image
                    predictG, predictLabel = D(fake_images)
                    lossG_adv = criterion_adv(predictG, realLabel) # Compare vs label = 1 (We want to optimize G to fool D, predictG must tend to 1)
                    lossG_aux = criterion_aux(predictLabel, gen_labels)
                    lossG = lossG_adv + lossG_aux
                    optimizerD.zero_grad()
                    optimizerG.zero_grad()
                    lossG.backward()
                    optimizerG.step()
            
            if 0 == 0:
                    print("Epoch: "+str(epoch)+"/"+str(num_epoch)+ "  -- Batch:"+ str(batch_idx+1)+"/"+str(total_step))
                    print("     GenLoss "+str(round(lossG.item(), 3))+ "  --  DiscLoss "+str(round(lossD.item(), 3)))
                    print("     D(x): "+str(round(real_score.mean().item(), 3))+ "  -- D(G(z)):"+str(round(fake_score.mean().item(), 3)))

def denorm(x):
    out = (x + 1) / 2
    return out.clamp(0, 1)

def froze_layer(layer):
    for para in layer.parameters():
        para.requires_grad = False

def activate_layer(layer):
    for para in layer.parameters():
        para.requires_grad = True